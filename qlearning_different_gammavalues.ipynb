{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f850a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from degree_freedom_queen import *\n",
    "from degree_freedom_king1 import *\n",
    "from degree_freedom_king2 import *\n",
    "from generate_game import *\n",
    "from Chess_env import *\n",
    "\n",
    "#SET BOARD SIZE\n",
    "size_board = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba0b5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "##INITIALISE A GAME \n",
    "env=Chess_Env(size_board)\n",
    "SS,XS,allowed_aS=env.Initialise_game()\n",
    "N_actions = len(allowed_aS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6cbcf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##IMPLEMENT EPSILON GREEDY POLICY\n",
    "def EpsilonGreedy_Policy(Q, epsilon):\n",
    "\n",
    "    rand_value=np.random.uniform(0,1)\n",
    "\n",
    "    rand_a=rand_value<epsilon\n",
    "    \n",
    "    aS,_=np.where(allowed_a==1)\n",
    "    if rand_a==True:\n",
    "        \n",
    "        a=np.random.choice(aS)\n",
    "\n",
    "    else:\n",
    "\n",
    "        maxQ=np.argmax(Q)\n",
    "        a = aS[maxQ]\n",
    "            \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e4e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALISE THE PARAMETERS OF YOUR NEURAL NETWORK AND...\n",
    "# PLEASE CONSIDER TO USE A MASK OF ONE FOR THE ACTION MADE AND ZERO OTHERWISE IF YOU ARE NOT USING VANILLA GRADIENT DESCENT...\n",
    "# WE SUGGEST A NETWORK WITH ONE HIDDEN LAYER WITH SIZE 200. \n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = []   \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        for i, _ in enumerate(self.layers):\n",
    "            forward = self.layers[i].forward(X)\n",
    "            X = forward          \n",
    "        return forward\n",
    "\n",
    "    def update_w(self, X, Y, learning_rate):\n",
    "        # Forward pass\n",
    "        for i, _ in enumerate(self.layers):\n",
    "            forward = self.layers[i].forward(input_val=X)\n",
    "            X = forward         \n",
    "        # Compute loss and first gradient\n",
    "        mse = MeanSquaredError(forward, Y)\n",
    "        error = mse.forward()\n",
    "        gradient = mse.backward()  \n",
    "        self.loss.append(error)\n",
    "         \n",
    "        # Backpropagation\n",
    "        for i, _ in reversed(list(enumerate(self.layers))):\n",
    "            if self.layers[i].type != 'Linear':\n",
    "                gradient = self.layers[i].backward(gradient)\n",
    "            else:\n",
    "                gradient, dW, dB = self.layers[i].backward(gradient)\n",
    "                self.layers[i].optimize(dW, dB, learning_rate)             \n",
    "        return error\n",
    "\n",
    "\n",
    "class Layer: #Layer abstract class\n",
    "    def __init__(self):\n",
    "        pass   \n",
    "    def __len__(self):\n",
    "        pass     \n",
    "    def __str__(self):\n",
    "        pass   \n",
    "    def forward(self):\n",
    "        pass   \n",
    "    def backward(self):\n",
    "        pass  \n",
    "    def optimize(self):\n",
    "        pass\n",
    "        \n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        np.random.seed(90)\n",
    "        self.weights = np.random.rand(output_dim, input_dim)\n",
    "        np.random.seed(90)\n",
    "        self.biases = np.random.rand(output_dim, 1)\n",
    "        self.type = 'Linear'\n",
    "    def __str__(self):\n",
    "        return f\"{self.type} Layer\"     \n",
    "    def forward(self, input_val):\n",
    "        self._prev_acti = input_val.reshape(self.weights.shape[1],1)\n",
    "        return np.matmul(self.weights, input_val).reshape(self.weights.shape[0],1) + self.biases   \n",
    "    def backward(self, dA):\n",
    "        dW = np.dot(dA, self._prev_acti.T)\n",
    "        dB = dA.mean(axis=1, keepdims=True)        \n",
    "        delta = np.dot(self.weights.T, dA)        \n",
    "        return delta, dW, dB    \n",
    "    def optimize(self, dW, dB, rate):\n",
    "        self.weights = self.weights - rate * dW\n",
    "        self.biases = self.biases - rate * dB  \n",
    "        \n",
    "class ReLU(Layer):    \n",
    "    def __init__(self, output_dim):\n",
    "        self.units = output_dim\n",
    "        self.type = 'ReLU'\n",
    "    def __str__(self):\n",
    "        return f\"{self.type} Layer\"               \n",
    "    def forward(self, input_val):\n",
    "        self._prev_acti = np.maximum(0, input_val)\n",
    "        return self._prev_acti    \n",
    "    def backward(self, dJ):\n",
    "        return dJ * np.heaviside(self._prev_acti, 0)\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self, output_dim):\n",
    "        self.units = output_dim\n",
    "        self.type = 'Sigmoid'\n",
    "    def __str__(self):\n",
    "        return f\"{self.type} Layer\"                \n",
    "    def forward(self, input_val):\n",
    "        self._prev_acti = 1 / (1 + np.exp(-input_val))\n",
    "        return self._prev_acti    \n",
    "    def backward(self, dJ):\n",
    "        sig = self._prev_acti\n",
    "        return dJ * sig * (1 - sig)\n",
    "    \n",
    "class MeanSquaredError(Layer):\n",
    "    def __init__(self, predicted, real):\n",
    "        self.predicted = predicted\n",
    "        self.real = real\n",
    "        self.type = 'Mean Squared Error'\n",
    "    def forward(self):\n",
    "        return np.power(self.predicted - self.real, 2).mean()\n",
    "    def backward(self):\n",
    "        return 2 * (self.predicted - self.real)/self.predicted.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0cf8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS SUGGESTED (FOR A GRID SIZE OF 4)\n",
    "\n",
    "epsilon_0 = 0.2     # STARTING VALUE OF EPSILON FOR THE EPSILON-GREEDY POLICY\n",
    "beta = 0.00005      # THE PARAMETER SETS HOW QUICKLY THE VALUE OF EPSILON IS DECAYING (SEE epsilon_f BELOW)\n",
    "gamma = 0.85        # THE DISCOUNT FACTOR\n",
    "gamma_2 = 0.9         #Here we try different Gamma Values\n",
    "gamma_3 = 0.7 \n",
    "gamma_4 = 0.6\n",
    "eta = 0.0035        # THE LEARNING RATE\n",
    "\n",
    "\n",
    "N_episodes=40000 # THE NUMBER OF GAMES TO BE PLAYED \n",
    "\n",
    "# VARIABLES WHERE TO SAVE THE FINAL REWARD IN AN EPISODE AND THE NUMBER OF MOVES \n",
    "R_save_ql = np.zeros([N_episodes, 1])\n",
    "N_moves_save_ql = np.zeros([N_episodes, 1])\n",
    "R_save_ql_gamma_2 = np.zeros([N_episodes, 1])\n",
    "N_moves_save_ql_gamma_2 = np.zeros([N_episodes, 1])\n",
    "R_save_ql_gamma_3= np.zeros([N_episodes, 1])\n",
    "N_moves_save_ql_gamma_3 = np.zeros([N_episodes, 1])\n",
    "R_save_ql_gamma_4 = np.zeros([N_episodes, 1])\n",
    "N_moves_save_ql_gamma_4 = np.zeros([N_episodes, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad7597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPONENTIAL MOVING AVERAGE: B = [0,1]\n",
    "def EMA(Y,B):\n",
    "    y = 0\n",
    "    EMAY = []\n",
    "    for n in Y:\n",
    "        y = B * y + (1 - B) * n\n",
    "        EMAY.append(y.copy())\n",
    "    return EMAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051d9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "## NUMBER OF HIDDEN NODES\n",
    "N_h=200\n",
    "# Create model\n",
    "model = Model() \n",
    "# Add layers\n",
    "model.add(Linear(np.shape(XS)[0], N_h))\n",
    "model.add(ReLU(N_h))\n",
    "model.add(Linear(N_h,32))\n",
    "model.add(ReLU(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3238ab61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have Finished 0 Epoachs\n",
      "Have Finished 500 Epoachs\n",
      "Have Finished 1000 Epoachs\n",
      "Have Finished 1500 Epoachs\n",
      "Have Finished 2000 Epoachs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m a_agent_next \u001b[38;5;241m=\u001b[39m EpsilonGreedy_Policy(Qvaluesactions_next,\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     30\u001b[0m Qvaluesall[a_agent] \u001b[38;5;241m=\u001b[39m R \u001b[38;5;241m+\u001b[39m gamma\u001b[38;5;241m*\u001b[39mQvaluesall_next[a_agent_next]\n\u001b[1;32m---> 31\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mQvaluesall\u001b[49m\u001b[43m,\u001b[49m\u001b[43meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m S\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mcopy(S_next)\n\u001b[0;32m     34\u001b[0m X\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mcopy(X_next)\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mModel.update_w\u001b[1;34m(self, X, Y, learning_rate)\u001b[0m\n\u001b[0;32m     32\u001b[0m         gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mbackward(gradient)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m         gradient, dW, dB \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39moptimize(dW, dB, learning_rate)             \n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mLinear.backward\u001b[1;34m(self, dA)\u001b[0m\n\u001b[0;32m     66\u001b[0m dW \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(dA, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_acti\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     67\u001b[0m dB \u001b[38;5;241m=\u001b[39m dA\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)        \n\u001b[1;32m---> 68\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdA\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m delta, dW, dB\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TRAINING STRUCTURE\n",
    "for n in range(N_episodes):\n",
    "    epsilon_f = epsilon_0 / (1 + beta * n)   ## DECAYING EPSILON\n",
    "    S,X,allowed_a=env.Initialise_game()     # INITIALISE GAME\n",
    "    Done=0                                  # SET Done=0 AT THE BEGINNING\n",
    "    i=1                                     # COUNTER FOR THE NUMBER OF ACTIONS (MOVES) IN AN EPISODE\n",
    "    \n",
    "    # UNTIL THE EPISODE IS NOT OVER...(Done=0)\n",
    "    while Done==0:\n",
    "        Qvaluesall = model.predict(X.reshape(58,1))\n",
    "        Qvaluesactions = Qvaluesall[np.where(allowed_a==1)[0]]\n",
    "        a_agent=EpsilonGreedy_Policy(Qvaluesactions, epsilon_f)\n",
    "        S_next,X_next,allowed_a_next,R,Done=env.OneStep(a_agent)\n",
    "        \n",
    "        if Done:\n",
    "            \n",
    "            R_save_ql[n]=np.copy(R)\n",
    "            N_moves_save_ql[n]=np.copy(i)\n",
    "            Qvaluesall[a_agent] = R\n",
    "            model.update_w(X,Qvaluesall,eta)\n",
    "            break\n",
    "            \n",
    "        # IF THE EPISODE IS NOT OVER...    \n",
    "        else:\n",
    "            \n",
    "            # NEXT STATE AND CO. BECOME ACTUAL STATE... \n",
    "            Qvaluesall_next = model.predict(X_next.reshape(58,1)) \n",
    "            Qvaluesactions_next = Qvaluesall_next[np.where(allowed_a==1)[0]]\n",
    "            a_agent_next = EpsilonGreedy_Policy(Qvaluesactions_next,0)\n",
    "            Qvaluesall[a_agent] = R + gamma*Qvaluesall_next[a_agent_next]\n",
    "            model.update_w(X,Qvaluesall,eta)\n",
    "\n",
    "            S=np.copy(S_next)\n",
    "            X=np.copy(X_next)\n",
    "            allowed_a=np.copy(allowed_a_next)\n",
    "\n",
    "        # UPDATE THE COUNTER\n",
    "        i=i+1\n",
    "    #call back\n",
    "    if n%500 ==0:\n",
    "        print(\"Have Finished \"+ str(n)+\" Epoachs\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07493a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "## NUMBER OF HIDDEN NODES\n",
    "N_h=200\n",
    "# Create model\n",
    "model_2 = Model() \n",
    "# Add layers\n",
    "model_2.add(Linear(np.shape(XS)[0], N_h))\n",
    "model_2.add(ReLU(N_h))\n",
    "model_2.add(Linear(N_h,32))\n",
    "model_2.add(ReLU(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b457e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING STRUCTURE\n",
    "for n in range(N_episodes):\n",
    "    epsilon_f = epsilon_0 / (1 + beta * n)   ## DECAYING EPSILON\n",
    "    S,X,allowed_a=env.Initialise_game()     # INITIALISE GAME\n",
    "    Done=0                                  # SET Done=0 AT THE BEGINNING\n",
    "    i=1                                     # COUNTER FOR THE NUMBER OF ACTIONS (MOVES) IN AN EPISODE\n",
    "    \n",
    "    # UNTIL THE EPISODE IS NOT OVER...(Done=0)\n",
    "    while Done==0:\n",
    "        Qvaluesall = model_2.predict(X.reshape(58,1))\n",
    "        Qvaluesactions = Qvaluesall[np.where(allowed_a==1)[0]]\n",
    "        a_agent=EpsilonGreedy_Policy(Qvaluesactions, epsilon_f)\n",
    "        S_next,X_next,allowed_a_next,R,Done=env.OneStep(a_agent)\n",
    "        \n",
    "        if Done:\n",
    "            \n",
    "            R_save_ql_gamma_2[n]=np.copy(R)\n",
    "            N_moves_save_ql_gamma_2[n]=np.copy(i)\n",
    "            Qvaluesall[a_agent] = R\n",
    "            model_2.update_w(X,Qvaluesall,eta)\n",
    "            break\n",
    "            \n",
    "        # IF THE EPISODE IS NOT OVER...    \n",
    "        else:\n",
    "            \n",
    "            # NEXT STATE AND CO. BECOME ACTUAL STATE... \n",
    "            Qvaluesall_next = model_2.predict(X_next.reshape(58,1)) \n",
    "            Qvaluesactions_next = Qvaluesall_next[np.where(allowed_a==1)[0]]\n",
    "            a_agent_next = EpsilonGreedy_Policy(Qvaluesactions_next,0)\n",
    "            Qvaluesall[a_agent] = R + gamma_2*Qvaluesall_next[a_agent_next]\n",
    "            model_2.update_w(X,Qvaluesall,eta)\n",
    "\n",
    "            S=np.copy(S_next)\n",
    "            X=np.copy(X_next)\n",
    "            allowed_a=np.copy(allowed_a_next)\n",
    "\n",
    "        # UPDATE THE COUNTER\n",
    "        i=i+1\n",
    "    #call back\n",
    "    if n%500 ==0:\n",
    "        print(\"Have Finished \"+ str(n)+\" Epoachs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba9b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "## NUMBER OF HIDDEN NODES\n",
    "N_h=200\n",
    "# Create model\n",
    "model_3 = Model() \n",
    "# Add layers\n",
    "model_3.add(Linear(np.shape(XS)[0], N_h))\n",
    "model_3.add(ReLU(N_h))\n",
    "model_3.add(Linear(N_h,32))\n",
    "model_3.add(ReLU(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5351341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING STRUCTURE\n",
    "for n in range(N_episodes):\n",
    "    epsilon_f = epsilon_0 / (1 + beta * n)   ## DECAYING EPSILON\n",
    "    S,X,allowed_a=env.Initialise_game()     # INITIALISE GAME\n",
    "    Done=0                                  # SET Done=0 AT THE BEGINNING\n",
    "    i=1                                     # COUNTER FOR THE NUMBER OF ACTIONS (MOVES) IN AN EPISODE\n",
    "    \n",
    "    # UNTIL THE EPISODE IS NOT OVER...(Done=0)\n",
    "    while Done==0:\n",
    "        Qvaluesall = model_3.predict(X.reshape(58,1))\n",
    "        Qvaluesactions = Qvaluesall[np.where(allowed_a==1)[0]]\n",
    "        a_agent=EpsilonGreedy_Policy(Qvaluesactions, epsilon_f)\n",
    "        S_next,X_next,allowed_a_next,R,Done=env.OneStep(a_agent)\n",
    "        \n",
    "        if Done:\n",
    "            \n",
    "            R_save_ql_gamma_3[n]=np.copy(R)\n",
    "            N_moves_save_ql_gamma_3[n]=np.copy(i)\n",
    "            Qvaluesall[a_agent] = R\n",
    "            model_3.update_w(X,Qvaluesall,eta)\n",
    "            break\n",
    "            \n",
    "        # IF THE EPISODE IS NOT OVER...    \n",
    "        else:\n",
    "            \n",
    "            # NEXT STATE AND CO. BECOME ACTUAL STATE... \n",
    "            Qvaluesall_next = model_3.predict(X_next.reshape(58,1)) \n",
    "            Qvaluesactions_next = Qvaluesall_next[np.where(allowed_a==1)[0]]\n",
    "            a_agent_next = EpsilonGreedy_Policy(Qvaluesactions_next, 0)\n",
    "            Qvaluesall[a_agent] = R + gamma_3*Qvaluesall_next[a_agent_next]\n",
    "            model_3.update_w(X,Qvaluesall,eta)\n",
    "\n",
    "            S=np.copy(S_next)\n",
    "            X=np.copy(X_next)\n",
    "            allowed_a=np.copy(allowed_a_next)\n",
    "\n",
    "        # UPDATE THE COUNTER\n",
    "        i=i+1\n",
    "    #call back\n",
    "    if n%500 ==0:\n",
    "        print(\"Have Finished \"+ str(n)+\" Epoachs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29420480",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INITALISE YOUR NEURAL NETWORK...\n",
    "## NUMBER OF HIDDEN NODES\n",
    "N_h=200\n",
    "# Create model\n",
    "model_4 = Model() \n",
    "# Add layers\n",
    "model_4.add(Linear(np.shape(XS)[0], N_h))\n",
    "model_4.add(ReLU(N_h))\n",
    "model_4.add(Linear(N_h,32))\n",
    "model_4.add(ReLU(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6686f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING STRUCTURE\n",
    "for n in range(N_episodes):\n",
    "    epsilon_f = epsilon_0 / (1 + beta * n)   ## DECAYING EPSILON\n",
    "    S,X,allowed_a=env.Initialise_game()     # INITIALISE GAME\n",
    "    Done=0                                  # SET Done=0 AT THE BEGINNING\n",
    "    i=1                                     # COUNTER FOR THE NUMBER OF ACTIONS (MOVES) IN AN EPISODE\n",
    "    \n",
    "    # UNTIL THE EPISODE IS NOT OVER...(Done=0)\n",
    "    while Done==0:\n",
    "        Qvaluesall = model_4.predict(X.reshape(58,1))\n",
    "        Qvaluesactions = Qvaluesall[np.where(allowed_a==1)[0]]\n",
    "        a_agent=EpsilonGreedy_Policy(Qvaluesactions, epsilon_f)\n",
    "        S_next,X_next,allowed_a_next,R,Done=env.OneStep(a_agent)\n",
    "        \n",
    "        if Done:\n",
    "            \n",
    "            R_save_ql_gamma_4[n]=np.copy(R)\n",
    "            N_moves_save_ql_gamma_4[n]=np.copy(i)\n",
    "            Qvaluesall[a_agent] = R\n",
    "            model_4.update_w(X,Qvaluesall,eta)\n",
    "            break\n",
    "            \n",
    "        # IF THE EPISODE IS NOT OVER...    \n",
    "        else:\n",
    "            \n",
    "            # NEXT STATE AND CO. BECOME ACTUAL STATE... \n",
    "            Qvaluesall_next = model_4.predict(X_next.reshape(58,1)) \n",
    "            Qvaluesactions_next = Qvaluesall_next[np.where(allowed_a==1)[0]]\n",
    "            a_agent_next = EpsilonGreedy_Policy(Qvaluesactions_next, 0)\n",
    "            Qvaluesall[a_agent] = R + gamma_4*Qvaluesall_next[a_agent_next]\n",
    "            model_4.update_w(X,Qvaluesall,eta)\n",
    "\n",
    "            S=np.copy(S_next)\n",
    "            X=np.copy(X_next)\n",
    "            allowed_a=np.copy(allowed_a_next)\n",
    "\n",
    "        # UPDATE THE COUNTER\n",
    "        i=i+1\n",
    "    #call back\n",
    "    if n%500 ==0:\n",
    "        print(\"Have Finished \"+ str(n)+\" Epoachs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ab894",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT THE TOTAL REWARD PER EPISODE AS TRAINING PROGRESSES\n",
    "plt.figure()\n",
    "plt.plot(EMA(R_save_ql,0.999), color='green', label=\"gamma = 0.85\")\n",
    "plt.plot(EMA(R_save_ql_gamma_2,0.999), color='red', label=\"gamma = 0.9\")\n",
    "plt.plot(EMA(R_save_ql_gamma_3,0.999), color='black', label=\"gamma = 0.7\")\n",
    "plt.plot(EMA(R_save_ql_gamma_4,0.999), color='blue', label=\"gamma = 0.6\")\n",
    "plt.xlabel('Episode Number')\n",
    "plt.ylabel('Reward')\n",
    "plt.title(\"Q-Learning\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT THE NUMBER OF MOVES AS TRAINING PROGRESSES\n",
    "plt.figure()\n",
    "plt.plot(EMA(N_moves_save_ql,0.9995), color='green', label=\"gamma = 0.85\")\n",
    "#plt.plot(EMA(N_moves_save_ql_gamma_2,0.9995), color='red', label=\"gamma = 0.9\")\n",
    "plt.plot(EMA(N_moves_save_ql_gamma_3,0.9995), color='black', label=\"gamma = 0.7\")\n",
    "plt.plot(EMA(N_moves_save_ql_gamma_4,0.9995), color='blue', label=\"gamma = 0.6\")\n",
    "plt.xlabel('Episode Number')\n",
    "plt.ylabel('Number of Moves')\n",
    "plt.title(\"Q-Learning\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fd91d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
